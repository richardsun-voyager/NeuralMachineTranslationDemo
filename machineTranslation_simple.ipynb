{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine translation is a challenge for computers not to only understand human languages but also to generate languages. A machine translation can be viewed as a conditional language model, given a source sentence $x_i$, we needed to calculate the probability of generated sentence $p(y_i|x_i)$. In early years, statistical machine translation(SMT) was a focus, amongst which IBM models were basis, if you are interested, please visit Michael Collins' [webpage](http://www.cs.columbia.edu/~mcollins/), there he provided many useful and explicit lecture notes to illustrate basis terms and models of SMT.\n",
    "\n",
    "In recent years, with the development of artificial neural networks as well as deep learning applications, neural translation models were explored, especially [seq2seq](https://arxiv.org/pdf/1406.1078v3.pdf) model as well as later models has improved performances of machine translation.\n",
    "\n",
    "This project aims to realize a simple neural machine translation model through seq2seq concept, namely we only transform the source sentence into a fixed vevctor as context input for decoder.  More information, please read KyunghyunCho's article [Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078v3.pdf) .\n",
    "\n",
    "Actually, there are some [excellent blogs](http://blog.csdn.net/u011414416/article/details/51048994) in Chinese, which introduced the development and theoretic models of neural machine translation explicitly and systemamtically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the paths of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Parameters\n",
    "data_dir = 'temp'\n",
    "data_file = 'eng_ger.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make data directory\n",
    "import os\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquiry\n",
    "\n",
    "Download the data from website if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class dataReader:\n",
    "    '''\n",
    "    Read text files from local drive.\n",
    "    If not exists, download it.\n",
    "    '''\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        if not self.__checkExists():\n",
    "            self.__download()\n",
    "        \n",
    "            \n",
    "    def loadData(self):\n",
    "        #print('Data Exists!')\n",
    "        eng_ger_data = []\n",
    "        with open(self.file_path, 'r') as in_conn:\n",
    "            for row in in_conn:\n",
    "                eng_ger_data.append(row[:-1])\n",
    "        return eng_ger_data\n",
    "\n",
    "    def __download(self):\n",
    "        '''Download text files which contain translation pairs'''\n",
    "        print('Data not found, downloading Eng-Ger sentences from www.manythings.org')\n",
    "        sentence_url = 'http://www.manythings.org/anki/deu-eng.zip'\n",
    "        r = urllib.request.urlopen(sentence_url)\n",
    "        z = ZipFile(io.BytesIO(r.read()))\n",
    "        file = z.read('deu.txt')\n",
    "        # Format Data\n",
    "        eng_ger_data = file.decode()\n",
    "        eng_ger_data = eng_ger_data.encode('ascii',errors='ignore')\n",
    "        eng_ger_data = eng_ger_data.decode().split('\\n')\n",
    "        # Write to file\n",
    "        with open(self.file_path, 'w') as out_conn:\n",
    "            for sentence in eng_ger_data:\n",
    "                out_conn.write(sentence + '\\n')\n",
    "                \n",
    "    def __checkExists(self):\n",
    "        '''Check the file'''\n",
    "        return os.path.isfile(self.file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dr = dataReader('eng_ger.txt')\n",
    "eng_ger_data = dr.loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi.\\tHallo!',\n",
       " 'Hi.\\tGr Gott!',\n",
       " 'Run!\\tLauf!',\n",
       " 'Wow!\\tPotzdonner!',\n",
       " 'Wow!\\tDonnerwetter!',\n",
       " 'Fire!\\tFeuer!',\n",
       " 'Help!\\tHilfe!',\n",
       " 'Help!\\tZu Hlf!',\n",
       " 'Stop!\\tStopp!',\n",
       " 'Wait!\\tWarte!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_ger_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processig\n",
    "\n",
    "Preprocess the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "vocab_size = 10000\n",
    "class textHandler:\n",
    "    '''Split sentences into pairs of Source-Target language'''\n",
    "    def __init__(self, data, vocab_size):\n",
    "        self.data = data\n",
    "        self.vocab_size = vocab_size\n",
    "        self.__sentSplit()\n",
    "        \n",
    "    def __removePunctuation(self):\n",
    "        '''Remove punctuation'''\n",
    "        # Remove punctuation\n",
    "        punct = string.punctuation\n",
    "        pair_data = [''.join(char for char in sent if char not in punct) for sent in self.data]\n",
    "        return pair_data\n",
    "    \n",
    "    def __sentSplit(self):\n",
    "        # Break each sentence pair by tabs, one part is English, the other is German. \n",
    "        pair_data = self.__removePunctuation()\n",
    "        s_t_data = [x.split('\\t') for x in pair_data if len(x)>=1]\n",
    "        [source_sentence, target_sentence] = [list(x) for x in zip(*s_t_data)]\n",
    "        #Split each sentence into words\n",
    "        self.source_sentence = [x.lower().split() for x in source_sentence]\n",
    "        self.target_sentence = [x.lower().split() for x in target_sentence]\n",
    "        #return source_sentence, target_sentence\n",
    "    \n",
    "    def __buildVocab(self, sents):\n",
    "        '''Build Vocabulary for both languages'''\n",
    "        # Process the English Vocabulary\n",
    "        all_words = [word for sent in sents for word in sent]\n",
    "        #Count the frequency of English words\n",
    "        all_words_counts = Counter(all_words)\n",
    "        #Get the most frequent vocab_size words, left regarded as unknow\n",
    "        word_keys = [x[0] for x in all_words_counts.most_common(self.vocab_size-3)] \n",
    "        #Word to ID, set Starting token as 'SOS', ending token as 'EOS'\n",
    "        vocab2ix = dict(zip(word_keys, range(2,self.vocab_size)))\n",
    "        vocab2ix['SOS'] = 0\n",
    "        vocab2ix['EOS'] = 1\n",
    "        vocab2ix['UNK'] = self.vocab_size - 1\n",
    "        #ID to Word\n",
    "        ix2vocab = {val:key for key, val in vocab2ix.items()}\n",
    "        return vocab2ix, ix2vocab\n",
    "    \n",
    "    def getSents(self):\n",
    "        '''Get preprocessed sentences'''\n",
    "        return self.source_sentence, self.target_sentence\n",
    "    \n",
    "    def sent2vec(self, sents, vocab2ix):\n",
    "        '''Transform sentences into Ids'''\n",
    "        processed = []\n",
    "        for sent in sents:\n",
    "            temp_sentence = []\n",
    "            for word in sent:\n",
    "                try:\n",
    "                    temp_sentence.append(vocab2ix[word])\n",
    "                except:\n",
    "                    #Unknown words\n",
    "                    temp_sentence.append(self.vocab_size-1)\n",
    "            processed.append(temp_sentence)\n",
    "        return processed\n",
    "    \n",
    "    def generateVocab(self):\n",
    "        '''Generate Vocabulary'''\n",
    "        #source_sentence, target_sentence = self.__sentSplit()\n",
    "        source_vocab2ix, source_ix2vocab = self.__buildVocab(self.source_sentence)\n",
    "        target_vocab2ix, target_ix2vocab = self.__buildVocab(self.target_sentence)\n",
    "        return source_vocab2ix, source_ix2vocab, target_vocab2ix, target_ix2vocab\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "th = textHandler(data=eng_ger_data, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_sentence, german_sentence = th.getSents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eng_vocab2ix, eng_ix2vocab, ger_vocab2ix, ger_ix2vocab = th.generateVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_processed = th.sent2vec(english_sentence, eng_vocab2ix)\n",
    "german_processed = th.sent2vec(german_sentence, ger_vocab2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = ['I love this dog', 'What a nice day', 'This is a book']\n",
    "test_data = [x.lower().split() for x in test_data]\n",
    "test_data = th.sent2vec(test_data, eng_vocab2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 168, 17, 191], [24, 7, 392, 117], [17, 8, 7, 123]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Build a simple encoder-decoder architecture\n",
    "\n",
    "In this demo, we use a simple encoder-decoder architecture to train and infer translations. We encode all the word vectors in source sentences into a fixed vector, then make the fixed vector as an input for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #Get embedding series of input words\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        #Compress the input vectors into RNN\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        #Create a initial zero hidden state\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        #result = result.cuda() if use_cuda else result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        #Create embedding\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #Transform input word id into embedding\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        #Generate output through input and last hidden state\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        #Create a initial zero hidden state\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        #result = result.cuda() if use_cuda else result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "In order to understand the mechanism of neural machine translation, we wrap a pair of translation sentences each time instead of a batch of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "max_length = 10\n",
    "encoder1 = EncoderRNN(vocab_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 100\n",
    "input_variable, target_variable = english_processed[index], german_processed[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform the input data and target into Variable vectors\n",
    "input_variable = Variable(torch.LongTensor(input_variable).view(-1, 1))\n",
    "target_variable = Variable(torch.LongTensor(target_variable).view(-1, 1))\n",
    "input_length = input_variable.size()[0]\n",
    "target_length = target_variable.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "We can compress a sereis of word embeddings into a final hidden state and output through RNN.\n",
    "$$h_t = f(h_{t-1}, x_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_hidden = encoder1.initHidden()\n",
    "encoder_outputs = Variable(torch.zeros(max_length, encoder1.hidden_size))\n",
    "#Calculate the final state of input words\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder1(\n",
    "        input_variable[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder without Attention\n",
    "\n",
    "In the decoder part,for training, we only take two inputs into consideration: The first is the target variables provided, and the second is the previous hidden state initialized by the final state($C_T$) of the encoder.\n",
    "$$h_t = f(h_{t-1}, y_{t-1}), h_0=C_T$$\n",
    "\n",
    "The architecture is as below(quoted from http://blog.csdn.net/jerr__y/article/details/53749693):\n",
    "![encoder-decoder](http://img.blog.csdn.net/20170509152556448?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSmVycl9feQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create an instance for decoder\n",
    "decoder1 = DecoderRNN(hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder1.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = 0\n",
    "criterion = nn.NLLLoss()\n",
    "decoder_input = Variable(torch.LongTensor([[0]]))\n",
    "#Set the beginning hidden state of decoder as the final state of encoder\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(target_length):\n",
    "    decoder_output, decoder_hidden = decoder1(\n",
    "        decoder_input, decoder_hidden)\n",
    "    loss += criterion(decoder_output[0], target_variable[di])\n",
    "    #Set the target as input\n",
    "    decoder_input = target_variable[di]  # Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 18.3088\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap it up\n",
    "\n",
    "Now, we can put the training procedures in one function. Note, in the paper [Sequence to Sequence Learning with Neural Networks\n",
    "](https://arxiv.org/pdf/1409.3215.pdf), they mentioned that it would be more efficient if we reversed the order of words in source sentences because the encoder could retain more information of the last few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "compressed = list(zip(english_processed, german_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_pairs = copy.deepcopy(compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def appendTokens(s):\n",
    "    '''Add ending tokens to each sentence'''\n",
    "    s.append(1)\n",
    "    return s\n",
    "\n",
    "def reverseSent(sent):\n",
    "    '''Reverse the order of words in a sentence'''\n",
    "    sent = list(reversed(sent))\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Filter those long sentences\n",
    "pairs_filtered = []\n",
    "#Because we need to add one ending tokens later, so substract 1 here\n",
    "np.random.shuffle(sent_pairs)\n",
    "for item in sent_pairs[:10000]:\n",
    "    if len(item[0]) <= (max_length-1) and len(item[0]) > 3:\n",
    "        s, t = item[0], item[1]\n",
    "        s = reverseSent(s)\n",
    "        pairs_filtered.append((s, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "def training(encoder, decoder, encoder_optimizer, decoder_optimizer, epochs=1):\n",
    "    for e in range(epochs):\n",
    "        np.random.shuffle(pairs_filtered)\n",
    "        for c, pair in enumerate(pairs_filtered):\n",
    "            #Add ending tokens for each pair\n",
    "            input_data, target_data = pair[0], pair[1]\n",
    "            input_data.append(1)\n",
    "            target_data.append(1)\n",
    "            #Transform the input data and target into Variable vectors\n",
    "            input_variable = Variable(torch.LongTensor(input_data).view(-1, 1))\n",
    "            target_variable = Variable(torch.LongTensor(target_data).view(-1, 1))\n",
    "            input_length = input_variable.size()[0]\n",
    "            target_length = target_variable.size()[0]\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "            #Calculate the final state of input words\n",
    "            for i in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(\n",
    "                    input_variable[i], encoder_hidden)\n",
    "                if i == max_length:\n",
    "                    print(c, pair)\n",
    "                encoder_outputs[i] = encoder_output[0][0]\n",
    "            #Clear grads\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            decoder_input = Variable(torch.LongTensor([[0]]))\n",
    "            #Set the beginning hidden state of decoder as the final state of encoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "                #print(decoder_output[0].size())\n",
    "                #print('*'*20)\n",
    "                #print(target_variable[di])\n",
    "                loss += criterion(decoder_output[0], target_variable[di])\n",
    "                #Set the target as input\n",
    "                decoder_input = target_variable[di]  # Teacher forcing\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            if c%200 == 0:\n",
    "                print(loss.data[0] / target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.28051503499349\n",
      "8.087350845336914\n",
      "7.310090065002441\n",
      "6.489105987548828\n",
      "6.605953216552734\n",
      "6.173790613810222\n",
      "7.359162466866629\n",
      "6.489985874720982\n",
      "5.146819591522217\n",
      "6.609954410129124\n",
      "5.694840567452567\n",
      "7.703678894042969\n",
      "7.36232328414917\n",
      "5.88268062046596\n",
      "5.424562454223633\n",
      "6.668548107147217\n",
      "7.05186398824056\n",
      "7.121327877044678\n",
      "5.375429630279541\n",
      "6.207205295562744\n",
      "5.655857563018799\n",
      "6.4616539001464846\n",
      "5.991965611775716\n",
      "6.0854900905064175\n",
      "6.7501373291015625\n",
      "4.970180892944336\n",
      "4.9261124928792315\n",
      "6.565703392028809\n",
      "5.912744522094727\n",
      "5.052161625453404\n",
      "7.354418667879972\n",
      "5.72467041015625\n",
      "4.825438499450684\n",
      "6.486326217651367\n",
      "5.998085021972656\n",
      "5.692766462053571\n",
      "4.788918813069661\n",
      "5.158510843912761\n",
      "4.944438298543294\n",
      "4.610002899169922\n",
      "4.6514303419325085\n"
     ]
    }
   ],
   "source": [
    "encoder1 = EncoderRNN(vocab_size, hidden_size)\n",
    "decoder1 = DecoderRNN(hidden_size, vocab_size)\n",
    "encoder1_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate)\n",
    "decoder1_optimizer = optim.SGD(decoder1.parameters(), lr=learning_rate)\n",
    "training(encoder1, decoder1, encoder1_optimizer, decoder1_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And next, we use a greedy method to generate a target sentence based on source sentence. Each time, we selected the word wich has the maximum probability untile the decoder generate an ending token 'EOS'. However, a best choice each time does not guarantte the most-likely sentence in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_greedy(encoder, decoder, sentence, max_length=10):\n",
    "    input_variable = Variable(torch.LongTensor(sentence).view(-1, 1))\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    #encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        #encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "    \n",
    "    #Set the inital value as SOS token\n",
    "    decoder_input = Variable(torch.LongTensor([[0]]))  # SOS\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoded_words = []\n",
    "\n",
    "    #Greedy method\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        top_value, top_index = decoder_output.data.topk(1)\n",
    "        #Get the index of the word\n",
    "        ni = top_index[0][0]\n",
    "        if ni == 1:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(ger_ix2vocab[ni])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        #decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "\n",
    "    print(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beamsearch_topk(decoder, decoder_input, decoder_hidden, k=2):\n",
    "    '''Beam Search Method'''\n",
    "    decoder_output, decoder_hidden = decoder(\n",
    "        decoder_input, decoder_hidden)\n",
    "    top_value, top_index = decoder_output.data.topk(k)\n",
    "    #first, second = top_index[0][0], top_index[0][1]\n",
    "    return top_value, top_index, decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_beamsearch(encoder, decoder, sentence, max_length=10):\n",
    "    input_variable = Variable(torch.LongTensor(sentence).view(-1, 1))\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    #encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        #encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "    \n",
    "    #Set the inital value as SOS token\n",
    "    decoder_input = Variable(torch.LongTensor([[0]]))  # SOS\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "\n",
    "    \n",
    "    #Beam search method\n",
    "    #Get the largest k values and their indice\n",
    "    k = 2\n",
    "    top_value, top_index, decoder_hidden = beamsearch_topk(decoder, decoder_input, decoder_hidden)\n",
    "    log_sum = top_value[0]\n",
    "    current_index = top_index[0]\n",
    "    #Create a recorder to record path\n",
    "    record_path_dict = {}\n",
    "    for i in range(k):\n",
    "        record_path_dict[i] = []\n",
    "    \n",
    "    #recordd hidden state\n",
    "    decoder_hidden_dict = {}\n",
    "    for i in range(k):\n",
    "        decoder_hidden_dict[i] = decoder_hidden\n",
    "    for di in range(1, max_length):\n",
    "        previous_index = current_index\n",
    "        #Traverse each selected word\n",
    "        log_sum_list = []\n",
    "        k_top_list = []\n",
    "        for i, ix in enumerate(previous_index):\n",
    "            input_id = Variable(torch.LongTensor([[ix]]))\n",
    "            top_value_temp, top_index_temp, decoder_hidden_temp = beamsearch_topk(decoder, \n",
    "                                                                                  input_id, decoder_hidden_dict[i])\n",
    "            for j, v in enumerate(top_value_temp[0]):\n",
    "                temp_sum = log_sum[i] + v\n",
    "                #Record sums\n",
    "                log_sum_list.append(temp_sum)\n",
    "                #Record pairs\n",
    "                k_top_list.append([ix, top_index_temp[0][j], decoder_hidden_temp])\n",
    "                \n",
    "        sum_ix_pair = list(zip(log_sum_list, k_top_list))\n",
    "        sum_ix_pair.sort(reverse=True)\n",
    "        for i in range(k):\n",
    "            #Update selected word ID and log sums of probability\n",
    "            current_index[i] = sum_ix_pair[i][1][1]\n",
    "            log_sum[i] += sum_ix_pair[i][0]\n",
    "            #Record the path\n",
    "            record_path_dict[i].append(sum_ix_pair[i][1][:2])\n",
    "            decoder_hidden_dict[i] = sum_ix_pair[i][1][2]\n",
    "\n",
    "        #first, second = top_index[0][0], top_index[0][1]\n",
    "        #if first == 1:\n",
    "            #decoded_words.append('<EOS>')\n",
    "            #break\n",
    "        #else:\n",
    "    for i in range(k):\n",
    "        decoded_words = []\n",
    "        print('The ' + str(i) + ' word chain:')\n",
    "        for ix_pair in record_path_dict[i]:\n",
    "            decoded_words.append(ger_ix2vocab[ix_pair[0]])\n",
    "            decoded_words.append(ger_ix2vocab[ix_pair[1]])\n",
    "            if ix_pair[1] == 1:\n",
    "                break\n",
    "            #decoded_words.append(ger_ix2vocab[first])\n",
    "        print(decoded_words)\n",
    "\n",
    "        #decoder_input = Variable(torch.LongTensor([[first]]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 7, 392, 117]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sie', 'ist', 'UNK', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "evaluate_greedy(encoder1, decoder1, test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0 word chain:\n",
      "['ich', 'habe', 'habe', 'UNK', 'UNK', 'EOS']\n",
      "The 1 word chain:\n",
      "['sie', 'ist', 'habe', 'ein', 'UNK', 'UNK', 'UNK', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "evaluate_beamsearch(encoder1, decoder1, test_data[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:myPytorch]",
   "language": "python",
   "name": "conda-env-myPytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
