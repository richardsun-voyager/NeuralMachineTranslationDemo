{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine translation is a challenge for computers not to only understand human languages but also to generate languages. A machine translation can be viewed as a conditional language model, given a source sentence $x_i$, we needed to calculate the probability of generated sentence $p(y_i|x_i)$. In early years, statistical machine translation(SMT) was a focus, amongst which IBM models were basis, if you are interested, please visit Michael Collins' [webpage](http://www.cs.columbia.edu/~mcollins/), there he provided many useful and explicit lecture notes to illustrate basis terms and models of SMT.\n",
    "\n",
    "In recent years, with the development of artificial neural networks as well as deep learning applications, neural translation models were explored, especially [seq2seq](https://arxiv.org/pdf/1406.1078v3.pdf) model as well as later models has improved performances of machine translation.\n",
    "\n",
    "This project aims to realize a neural machine translation model through seq2seq concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the paths of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Parameters\n",
    "data_dir = 'temp'\n",
    "data_file = 'eng_ger.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test Translation from English (lowercase, no punct)\n",
    "test_english = ['hello where is my computer',\n",
    "                'the quick brown fox jumped over the lazy dog',\n",
    "                'is it going to rain tomorrow']\n",
    "\n",
    "\n",
    "# Make data directory\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquiry\n",
    "\n",
    "Download the data from website if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "class dataReader:\n",
    "    '''\n",
    "    Read text files from local drive.\n",
    "    If not exists, download it.\n",
    "    '''\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        if not self.__checkExists():\n",
    "            self.__download()\n",
    "        \n",
    "            \n",
    "    def loadData(self):\n",
    "        #print('Data Exists!')\n",
    "        eng_ger_data = []\n",
    "        with open(self.file_path, 'r') as in_conn:\n",
    "            for row in in_conn:\n",
    "                eng_ger_data.append(row[:-1])\n",
    "        return eng_ger_data\n",
    "\n",
    "    def __download(self):\n",
    "        '''Download text files which contain translation pairs'''\n",
    "        print('Data not found, downloading Eng-Ger sentences from www.manythings.org')\n",
    "        sentence_url = 'http://www.manythings.org/anki/deu-eng.zip'\n",
    "        r = urllib.request.urlopen(sentence_url)\n",
    "        z = ZipFile(io.BytesIO(r.read()))\n",
    "        file = z.read('deu.txt')\n",
    "        # Format Data\n",
    "        eng_ger_data = file.decode()\n",
    "        eng_ger_data = eng_ger_data.encode('ascii',errors='ignore')\n",
    "        eng_ger_data = eng_ger_data.decode().split('\\n')\n",
    "        # Write to file\n",
    "        with open(self.file_path, 'w') as out_conn:\n",
    "            for sentence in eng_ger_data:\n",
    "                out_conn.write(sentence + '\\n')\n",
    "                \n",
    "    def __checkExists(self):\n",
    "        '''Check the file'''\n",
    "        return os.path.isfile(self.file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dr = dataReader('eng_ger.txt')\n",
    "eng_ger_data = dr.loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi.\\tHallo!',\n",
       " 'Hi.\\tGr Gott!',\n",
       " 'Run!\\tLauf!',\n",
       " 'Wow!\\tPotzdonner!',\n",
       " 'Wow!\\tDonnerwetter!',\n",
       " 'Fire!\\tFeuer!',\n",
       " 'Help!\\tHilfe!',\n",
       " 'Help!\\tZu Hlf!',\n",
       " 'Stop!\\tStopp!',\n",
       " 'Wait!\\tWarte!']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_ger_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processig\n",
    "\n",
    "Preprocess the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "vocab_size = 10000\n",
    "class textHandler:\n",
    "    '''Split sentences into pairs of Source-Target language'''\n",
    "    def __init__(self, data, vocab_size):\n",
    "        self.data = data\n",
    "        self.vocab_size = vocab_size\n",
    "        self.__sentSplit()\n",
    "        \n",
    "    def __removePunctuation(self):\n",
    "        '''Remove punctuation'''\n",
    "        # Remove punctuation\n",
    "        punct = string.punctuation\n",
    "        pair_data = [''.join(char for char in sent if char not in punct) for sent in self.data]\n",
    "        return pair_data\n",
    "    \n",
    "    def __sentSplit(self):\n",
    "        # Break each sentence pair by tabs, one part is English, the other is German. \n",
    "        pair_data = self.__removePunctuation()\n",
    "        s_t_data = [x.split('\\t') for x in pair_data if len(x)>=1]\n",
    "        [source_sentence, target_sentence] = [list(x) for x in zip(*s_t_data)]\n",
    "        #Split each sentence into words\n",
    "        self.source_sentence = [x.lower().split() for x in source_sentence]\n",
    "        self.target_sentence = [x.lower().split() for x in target_sentence]\n",
    "        #return source_sentence, target_sentence\n",
    "    \n",
    "    def __buildVocab(self, sents):\n",
    "        '''Build Vocabulary for both languages'''\n",
    "        # Process the English Vocabulary\n",
    "        all_words = [word for sent in sents for word in sent]\n",
    "        #Count the frequency of English words\n",
    "        all_words_counts = Counter(all_words)\n",
    "        #Get the most frequent vocab_size words, left regarded as unknow\n",
    "        word_keys = [x[0] for x in all_words_counts.most_common(self.vocab_size-3)] \n",
    "        #Word to ID, set Starting token as 'SOS', ending token as 'EOS'\n",
    "        vocab2ix = dict(zip(word_keys, range(2,self.vocab_size-1)))\n",
    "        vocab2ix['SOS'] = 0\n",
    "        vocab2ix['EOS'] = 1\n",
    "        #ID to Word\n",
    "        ix2vocab = {val:key for key, val in vocab2ix.items()}\n",
    "        return vocab2ix, ix2vocab\n",
    "    \n",
    "    def getSents(self):\n",
    "        '''Get preprocessed sentences'''\n",
    "        return self.source_sentence, self.target_sentence\n",
    "    \n",
    "    def sent2vec(self, sents, vocab2ix):\n",
    "        '''Transform sentences into Ids'''\n",
    "        processed = []\n",
    "        for sent in sents:\n",
    "            temp_sentence = []\n",
    "            for word in sent:\n",
    "                try:\n",
    "                    temp_sentence.append(vocab2ix[word])\n",
    "                except:\n",
    "                    #Unknown words\n",
    "                    temp_sentence.append(self.vocab_size-1)\n",
    "            processed.append(temp_sentence)\n",
    "        return processed\n",
    "    \n",
    "    def generateVocab(self):\n",
    "        '''Generate Vocabulary'''\n",
    "        #source_sentence, target_sentence = self.__sentSplit()\n",
    "        source_vocab2ix, source_ix2vocab = self.__buildVocab(self.source_sentence)\n",
    "        target_vocab2ix, target_ix2vocab = self.__buildVocab(self.target_sentence)\n",
    "        return source_vocab2ix, source_ix2vocab, target_vocab2ix, target_ix2vocab\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "th = textHandler(data=eng_ger_data, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_sentence, german_sentence = th.getSents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_vocab2ix, eng_ix2vocab, ger_vocab2ix, ger_ix2vocab = th.generateVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_processed = th.sent2vec(english_sentence, eng_vocab2ix)\n",
    "german_processed = th.sent2vec(german_sentence, ger_vocab2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = ['I love this dog', 'What a nice day', 'This is a book']\n",
    "test_data = [x.lower().split() for x in test_data]\n",
    "test_data = th.sent2vec(test_data, eng_vocab2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 168, 17, 191], [24, 7, 392, 117], [17, 8, 7, 123]]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build a simple encoder-decoder architecture\n",
    "\n",
    "In this demo, we use a simple encoder-decoder architecture to train and infer translations. We encode all the word vectors in source sentences into a fixed vector, then make the fixed vector as an input for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #Get embedding series of input words\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        #Compress the input vectors into RNN\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        #Create a initial zero hidden state\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        result = result.cuda() if use_cuda else result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        #Create embedding\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #Transform input word id into embedding\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        #Generate output through input and last hidden state\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        #Create a initial zero hidden state\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        result = result.cuda() if use_cuda else result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "In order to understand the mechanism of neural machine translation, we wrap a pair of translation sentences each time instead of a batch of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "max_length = 10\n",
    "encoder1 = EncoderRNN(vocab_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 100\n",
    "input_variable, target_variable = english_processed[index], german_processed[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform the input data and target into Variable vectors\n",
    "input_variable = Variable(torch.LongTensor(input_variable).view(-1, 1))\n",
    "target_variable = Variable(torch.LongTensor(target_variable).view(-1, 1))\n",
    "input_length = input_variable.size()[0]\n",
    "target_length = target_variable.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "We can compress a sereis of word embeddings into a final hidden state and output through RNN.\n",
    "$$h_t = f(h_{t-1}, x_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_hidden = encoder1.initHidden()\n",
    "encoder_outputs = Variable(torch.zeros(max_length, encoder1.hidden_size)).cuda()\n",
    "#Calculate the final state of input words\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder1(\n",
    "        input_variable[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder without Attention\n",
    "\n",
    "In the decoder part,for training, we only take two inputs into consideration: The first is the target variables provided, and the second is the previous hidden state initialized by the final state($C_T$) of the encoder.\n",
    "$$h_t = f(h_{t-1}, y_{t-1}), h_0=C_T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create an instance for decoder\n",
    "decoder1 = DecoderRNN(hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder1.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = 0\n",
    "criterion = nn.NLLLoss()\n",
    "decoder_input = Variable(torch.LongTensor([[0]]))\n",
    "#Set the beginning hidden state of decoder as the final state of encoder\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(target_length):\n",
    "    decoder_output, decoder_hidden = decoder1(\n",
    "        decoder_input, decoder_hidden)\n",
    "    loss += criterion(decoder_output[0], target_variable[di])\n",
    "    #Set the target as input\n",
    "    decoder_input = target_variable[di]  # Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 18.2935\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap it up\n",
    "\n",
    "Now, we can put the training procedures in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compressed = list(zip(english_processed, german_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filter those long sentences\n",
    "pairs_filtered = []\n",
    "#Because we need to add one ending tokens later, so substract 1 here\n",
    "for item in compressed:\n",
    "    if len(item[0]) <= (max_length-1) and len(item[0]) > 3:\n",
    "        pairs_filtered.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122833"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "criterion = nn.NLLLoss()\n",
    "def training(encoder, decoder, encoder_optimizer, decoder_optimizer, epochs=1):\n",
    "    for e in range(epochs):\n",
    "        np.random.shuffle(pairs_filtered)\n",
    "        for c, pair in enumerate(pairs_filtered):\n",
    "            #Add ending tokens for each pair\n",
    "            input_data, target_data = pair[0], pair[1]\n",
    "            input_data.append(1)\n",
    "            target_data.append(1)\n",
    "            #Transform the input data and target into Variable vectors\n",
    "            input_variable = Variable(torch.LongTensor(input_data).view(-1, 1))\n",
    "            target_variable = Variable(torch.LongTensor(target_data).view(-1, 1))\n",
    "            input_length = input_variable.size()[0]\n",
    "            target_length = target_variable.size()[0]\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "            #Calculate the final state of input words\n",
    "            for i in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(\n",
    "                    input_variable[i], encoder_hidden)\n",
    "                if i == max_length:\n",
    "                    print(c, pair)\n",
    "                encoder_outputs[i] = encoder_output[0][0]\n",
    "            #Clear grads\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            decoder_input = Variable(torch.LongTensor([[0]]))\n",
    "            #Set the beginning hidden state of decoder as the final state of encoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "                #print(decoder_output[0].size())\n",
    "                #print('*'*20)\n",
    "                #print(target_variable[di])\n",
    "                loss += criterion(decoder_output[0], target_variable[di])\n",
    "                #Set the target as input\n",
    "                decoder_input = target_variable[di]  # Teacher forcing\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            if c%200 == 0:\n",
    "                print(loss.data[0] / target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.267947605678014\n",
      "7.598428998674665\n",
      "6.105644226074219\n",
      "5.3449232918875555\n",
      "7.253068542480468\n",
      "7.965466181437175\n",
      "6.930266571044922\n",
      "5.051567077636719\n",
      "6.856137084960937\n",
      "6.641273498535156\n",
      "6.831576824188232\n",
      "7.747750759124756\n",
      "7.481189727783203\n",
      "7.446408952985491\n",
      "4.015239715576172\n",
      "7.633642408582899\n",
      "6.758612738715278\n",
      "5.9428151448567705\n",
      "6.0509490966796875\n",
      "6.263080596923828\n",
      "6.491754531860352\n",
      "6.0975807189941404\n",
      "5.787358093261719\n",
      "5.547765731811523\n",
      "5.210450914171007\n",
      "5.621366228376116\n",
      "5.877564748128255\n",
      "6.181207275390625\n",
      "6.781126658121745\n",
      "6.061775970458984\n",
      "7.495883941650391\n",
      "2.7922122478485107\n",
      "5.699005550808376\n",
      "4.900494893391927\n",
      "4.520580927530925\n",
      "5.432256426130023\n",
      "5.0531158447265625\n",
      "6.049356460571289\n",
      "4.832286198933919\n",
      "3.9473625818888345\n",
      "4.982153574625651\n",
      "6.0291748046875\n",
      "6.019882678985596\n",
      "7.092292022705078\n",
      "4.728400230407715\n",
      "6.9780455695258246\n",
      "4.893213272094727\n",
      "6.640546625310725\n",
      "6.496949005126953\n",
      "6.048442459106445\n",
      "4.841963450113933\n",
      "6.238411903381348\n",
      "5.215022193060981\n",
      "5.563482284545898\n",
      "4.446040562220982\n",
      "6.467803107367621\n",
      "4.2657623291015625\n",
      "3.6705654689243863\n",
      "6.129754384358724\n",
      "5.309228079659598\n",
      "5.5461891174316404\n",
      "5.382827281951904\n",
      "7.477799987792968\n",
      "5.960202303799716\n",
      "4.460855484008789\n",
      "6.5648750305175785\n",
      "5.20890458424886\n",
      "4.816983116997613\n",
      "5.467135111490886\n",
      "5.831013488769531\n",
      "5.749010358537946\n",
      "5.302555847167969\n",
      "5.548667271931966\n",
      "5.6688493092854815\n",
      "6.479893578423394\n",
      "5.03523063659668\n",
      "6.311266762869699\n",
      "5.746524047851563\n",
      "4.9253213670518665\n",
      "3.9563812255859374\n",
      "5.437749862670898\n",
      "5.758288860321045\n",
      "4.917041354709202\n",
      "5.516290392194476\n",
      "4.614424705505371\n",
      "5.521040780203683\n",
      "6.92373784383138\n",
      "5.040232181549072\n",
      "5.225831349690755\n",
      "4.044150670369466\n",
      "5.8224059210883246\n",
      "6.227055443657769\n",
      "6.351171970367432\n",
      "5.39997058444553\n",
      "5.292182922363281\n",
      "5.02863883972168\n",
      "6.786534203423394\n",
      "5.0170847574869795\n",
      "5.114834785461426\n",
      "5.712262834821429\n",
      "6.365755558013916\n",
      "3.577896499633789\n",
      "5.917914072672526\n",
      "5.218301391601562\n",
      "5.974845886230469\n",
      "3.71640625\n",
      "5.059250831604004\n",
      "4.641134262084961\n",
      "4.325027084350586\n",
      "5.825360979352679\n",
      "4.700409412384033\n",
      "4.444184939066569\n",
      "5.55326897757394\n",
      "5.749343236287435\n",
      "5.694979667663574\n",
      "7.141797637939453\n",
      "6.17378044128418\n",
      "6.590909685407366\n",
      "3.7631168365478516\n",
      "4.8167366027832035\n",
      "5.8497516087123325\n",
      "5.72671365737915\n",
      "7.1643931070963545\n",
      "5.139840443929036\n",
      "6.836558024088542\n",
      "5.220260302225749\n",
      "5.19074821472168\n",
      "6.237836837768555\n",
      "6.555973900689019\n",
      "5.170572280883789\n",
      "5.3505063738141745\n",
      "5.201872253417969\n",
      "6.872989654541016\n",
      "6.177938842773438\n",
      "3.7372376124064126\n",
      "5.175831534645774\n",
      "4.98821792602539\n",
      "6.511849721272786\n",
      "5.5890990363226996\n",
      "4.676832962036133\n",
      "5.345757075718471\n",
      "6.096628189086914\n",
      "7.594511244032118\n",
      "5.26366493918679\n",
      "5.573897770472935\n",
      "6.840958186558315\n",
      "6.200306415557861\n",
      "5.6302032470703125\n",
      "5.301637967427571\n",
      "4.559668064117432\n",
      "4.018088150024414\n",
      "5.520593915666852\n",
      "4.797294616699219\n",
      "5.477187633514404\n",
      "5.056046077183315\n",
      "6.009508768717448\n",
      "5.631992633526142\n",
      "4.454669952392578\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-c5b00f48e7d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mencoder1_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdecoder1_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder1_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder1_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-153-ab3c89d47d75>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(encoder, decoder, encoder_optimizer, decoder_optimizer, epochs)\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[1;31m#Set the target as input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mdecoder_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Teacher forcing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gradient has to be a Tensor, Variable or None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder1 = EncoderRNN(vocab_size, hidden_size)\n",
    "decoder1 = DecoderRNN(hidden_size, vocab_size)\n",
    "encoder1_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate)\n",
    "decoder1_optimizer = optim.SGD(decoder1.parameters(), lr=learning_rate)\n",
    "training(encoder1, decoder1, encoder1_optimizer, decoder1_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=10):\n",
    "    input_variable = Variable(torch.LongTensor(sentence).view(-1, 1))\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    #encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        #encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "    \n",
    "    #Set the inital value as SOS token\n",
    "    decoder_input = Variable(torch.LongTensor([[0]]))  # SOS\n",
    "\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "\n",
    "\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == 1:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(ger_ix2vocab[ni])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        #decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    print(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 7, 392, 117]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tom', 'hat', 'sich', 'nicht', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "evaluate(encoder1, decoder1, test_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
