{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine translation is a challenge for computers not to only understand human languages but also to generate languages. A machine translation can be viewed as a conditional language model, given a source sentence $x_i$, we needed to calculate the probability of generated sentence $p(y_i|x_i)$. In early years, statistical machine translation(SMT) was a focus, amongst which IBM models were basis, if you are interested, please visit Michael Collins' [webpage](http://www.cs.columbia.edu/~mcollins/), there he provided many useful and explicit lecture notes to illustrate basis terms and models of SMT.\n",
    "\n",
    "In recent years, with the development of artificial neural networks as well as deep learning applications, neural translation models were explored, especially [seq2seq](https://arxiv.org/pdf/1406.1078v3.pdf) model as well as later models has improved performances of machine translation.\n",
    "\n",
    "This project aims to realize a neural machine translation model through seq2seq concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Creating Sequence to Sequence Models\n",
    "#-------------------------------------\n",
    "#  Here we show how to implement sequence to sequence models.\n",
    "#  Specifically, we will build an English to German translation model.\n",
    "#\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import urllib\n",
    "import io\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow as tf\n",
    "from zipfile import ZipFile\n",
    "from collections import Counter\n",
    "import data_utils\n",
    "#import seq2seq_model\n",
    "#from tensorflow.python.framework import ops\n",
    "#ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义训练中各种变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Model Parameters\n",
    "learning_rate = 0.1\n",
    "lr_decay_rate = 0.99\n",
    "lr_decay_every = 100\n",
    "max_gradient = 5.0\n",
    "batch_size = 50\n",
    "num_layers = 3\n",
    "rnn_size = 500\n",
    "layer_size = 512\n",
    "generations = 100\n",
    "vocab_size = 10000\n",
    "save_every = 1000\n",
    "eval_every = 500\n",
    "output_every = 50\n",
    "punct = string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the paths of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Parameters\n",
    "data_dir = 'temp'\n",
    "data_file = 'eng_ger.txt'\n",
    "model_path = 'seq2seq_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Translation from English (lowercase, no punct)\n",
    "test_english = ['hello where is my computer',\n",
    "                'the quick brown fox jumped over the lazy dog',\n",
    "                'is it going to rain tomorrow']\n",
    "\n",
    "# Make Model Directory\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)\n",
    "\n",
    "# Make data directory\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquiry\n",
    "\n",
    "Download the data from website if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = ZipFile('temp/deu-eng.zip', 'r')\n",
    "file = z.read('deu.txt')\n",
    "# Format Data\n",
    "eng_ger_data = file.decode()\n",
    "eng_ger_data = eng_ger_data.encode('ascii',errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_ger_data = eng_ger_data.decode().split('\\n')\n",
    "# Write to file\n",
    "with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "      for sentence in eng_ger_data:\n",
    "          out_conn.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading English-German Data\n",
      "Data Exists!\n"
     ]
    }
   ],
   "source": [
    "print('Loading English-German Data')\n",
    "# Check for data, if it doesn't exist, download it and save it\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Data not found, downloading Eng-Ger sentences from www.manythings.org')\n",
    "    sentence_url = 'http://www.manythings.org/anki/deu-eng.zip'\n",
    "    r = urllib.request.urlopen(sentence_url)\n",
    "    z = ZipFile(io.BytesIO(r.read()))\n",
    "    file = z.read('deu.txt')\n",
    "    # Format Data\n",
    "    eng_ger_data = file.decode()\n",
    "    eng_ger_data = eng_ger_data.encode('ascii',errors='ignore')\n",
    "    eng_ger_data = eng_ger_data.decode().split('\\n')\n",
    "    # Write to file\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        for sentence in eng_ger_data:\n",
    "            out_conn.write(sentence + '\\n')\n",
    "else:\n",
    "    print('Data Exists!')\n",
    "    eng_ger_data = []\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as in_conn:\n",
    "        for row in in_conn:\n",
    "            eng_ger_data.append(row[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi\\tHallo'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "eng_ger_data = [''.join(char for char in sent if char not in punct) for sent in eng_ger_data]\n",
    "eng_ger_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break each translation pair into English and German sentences. And split each sentences into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Break each sentence pair by tabs, one part is English, the other is French.   \n",
    "eng_ger_data = [x.split('\\t') for x in eng_ger_data if len(x)>=1]\n",
    "[english_sentence, german_sentence] = [list(x) for x in zip(*eng_ger_data)]\n",
    "#Split each sentence into words\n",
    "english_sentence = [x.lower().split() for x in english_sentence]\n",
    "german_sentence = [x.lower().split() for x in german_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['help'] ['zu', 'hlf']\n"
     ]
    }
   ],
   "source": [
    "print(english_sentence[7], german_sentence[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map each unique word into a unique ID. Note, we need to set the start token and ending tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS = 0\n",
    "EOS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the vocabularies.\n"
     ]
    }
   ],
   "source": [
    "print('Processing the vocabularies.')\n",
    "# Process the English Vocabulary\n",
    "all_english_words = [word for sentence in english_sentence for word in sentence]\n",
    "#Count the frequency of English words\n",
    "all_english_counts = Counter(all_english_words)\n",
    "#Get the most frequent vocab_size words, left regarded as unknow\n",
    "eng_word_keys = [x[0] for x in all_english_counts.most_common(vocab_size-3)] \n",
    "#Word to ID\n",
    "eng_vocab2ix = dict(zip(eng_word_keys, range(2,vocab_size-1)))\n",
    "eng_vocab2ix['SOS'] = 0\n",
    "eng_vocab2ix['EOS'] = 1\n",
    "#ID to Word\n",
    "eng_ix2vocab = {val:key for key, val in eng_vocab2ix.items()}\n",
    "english_processed = []\n",
    "for sent in english_sentence:\n",
    "    temp_sentence = []\n",
    "    for word in sent:\n",
    "        try:\n",
    "            temp_sentence.append(eng_vocab2ix[word])\n",
    "        except:\n",
    "            #Unknown words\n",
    "            temp_sentence.append(vocab_size-1)\n",
    "    english_processed.append(temp_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the sentences have been transformed into series of word IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process the German Vocabulary\n",
    "all_german_words = [word for sentence in german_sentence for word in sentence]\n",
    "all_german_counts = Counter(all_german_words)\n",
    "#Leave three ids for start, ending tokens and unknown words\n",
    "ger_word_keys = [x[0] for x in all_german_counts.most_common(vocab_size-3)]\n",
    "#Set 0, 1 as the ID of starting and ending tokens\n",
    "ger_vocab2ix = dict(zip(ger_word_keys, range(2,vocab_size-1)))\n",
    "ger_vocab2ix['SOS'] = 0\n",
    "ger_vocab2ix['EOS'] = 1\n",
    "ger_ix2vocab = {val:key for key, val in ger_vocab2ix.items()}\n",
    "german_processed = []\n",
    "for sent in german_sentence:\n",
    "    temp_sentence = []\n",
    "    for word in sent:\n",
    "        try:\n",
    "            temp_sentence.append(ger_vocab2ix[word])\n",
    "        except:\n",
    "            #Unknown words\n",
    "            temp_sentence.append(vocab_size-1)\n",
    "    german_processed.append(temp_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the English-German translation, within each pair, the English sentence and the German sentence do not have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process the test english sentences, use '0' if word not in our vocab\n",
    "test_data = []\n",
    "for sentence in test_english:\n",
    "    temp_sentence = []\n",
    "    for word in sentence.split(' '):\n",
    "        try:\n",
    "            temp_sentence.append(eng_vocab2ix[word])\n",
    "        except:\n",
    "            # Use '0' if the word isn't in our vocabulary\n",
    "            temp_sentence.append(vocab_size-1)\n",
    "    test_data.append(temp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(eng_vocab2ix.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build encoder-decoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #Get embedding series of input words\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        #Compress the input vectors into RNN\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        #Create a initial zero hidden state\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        #result = result.cuda() if use_cuda else result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        #Create embedding\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #Transform input word id into embedding\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        #Create a initial zero hidden state\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        #result = result.cuda() if use_cuda else result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "In order to understand the mechanism of neural machine translation, we wrap a pair of translation sentences each time instead of a batch of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "max_length = 10\n",
    "encoder1 = EncoderRNN(vocab_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 100\n",
    "input_variable, target_variable = english_processed[index], german_processed[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform the input data and target into Variable vectors\n",
    "input_variable = Variable(torch.LongTensor(input_variable).view(-1, 1))\n",
    "target_variable = Variable(torch.LongTensor(target_variable).view(-1, 1))\n",
    "input_length = input_variable.size()[0]\n",
    "target_length = target_variable.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "We can compress a sereis of word embeddings into a final hidden state and output through RNN.\n",
    "$$h_t = f(h_{t-1}, x_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_hidden = encoder1.initHidden()\n",
    "encoder_outputs = Variable(torch.zeros(max_length, encoder1.hidden_size))\n",
    "#Calculate the final state of input words\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder1(\n",
    "        input_variable[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder without Attention\n",
    "\n",
    "In the decoder part,for training, we only take two inputs into consideration: The first is the target variables provided, and the second is the previous hidden state initialized by the final state($C_T$) of the encoder.\n",
    "$$h_t = f(h_{t-1}, y_{t-1}), h_0=C_T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create an instance for decoder\n",
    "decoder1 = DecoderRNN(hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder1.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = 0\n",
    "criterion = nn.NLLLoss()\n",
    "decoder_input = Variable(torch.LongTensor([[0]]))\n",
    "#Set the beginning hidden state of decoder as the final state of encoder\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(target_length):\n",
    "    decoder_output, decoder_hidden = decoder1(\n",
    "        decoder_input, decoder_hidden)\n",
    "    loss += criterion(decoder_output[0], target_variable[di])\n",
    "    #Set the target as input\n",
    "    decoder_input = target_variable[di]  # Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 18.6042\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap it up\n",
    "\n",
    "Now, we can put the training procedures in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compressed = list(zip(english_processed, german_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filter those long sentences\n",
    "pairs_filtered = []\n",
    "#Because we need to add one ending tokens later, so substract 1 here\n",
    "for item in compressed:\n",
    "    if len(item[0]) <= (max_length-1) and len(item[0]) > 2:\n",
    "        pairs_filtered.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131431"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "criterion = nn.NLLLoss()\n",
    "def training(encoder, decoder, encoder_optimizer, decoder_optimizer, epochs=1):\n",
    "    for e in range(epochs):\n",
    "        np.random.shuffle(pairs_filtered)\n",
    "        for c, pair in enumerate(pairs_filtered):\n",
    "            #Add ending tokens for each pair\n",
    "            input_data, target_data = pair[0], pair[1]\n",
    "            input_data.append(1)\n",
    "            target_data.append(1)\n",
    "            #Transform the input data and target into Variable vectors\n",
    "            input_variable = Variable(torch.LongTensor(input_data).view(-1, 1))\n",
    "            target_variable = Variable(torch.LongTensor(target_data).view(-1, 1))\n",
    "            input_length = input_variable.size()[0]\n",
    "            target_length = target_variable.size()[0]\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "            #Calculate the final state of input words\n",
    "            for i in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(\n",
    "                    input_variable[i], encoder_hidden)\n",
    "                if i == max_length:\n",
    "                    print(c, pair)\n",
    "                encoder_outputs[i] = encoder_output[0][0]\n",
    "            #Clear grads\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            decoder_input = Variable(torch.LongTensor([[0]]))\n",
    "            #Set the beginning hidden state of decoder as the final state of encoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "                #print(decoder_output[0].size())\n",
    "                #print('*'*20)\n",
    "                #print(target_variable[di])\n",
    "                loss += criterion(decoder_output[0], target_variable[di])\n",
    "                #Set the target as input\n",
    "                decoder_input = target_variable[di]  # Teacher forcing\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            if c%200 == 0:\n",
    "                print(loss.data[0] / target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.21161346435547\n",
      "18.286376953125\n",
      "43.86820329938616\n",
      "33.79951477050781\n",
      "44.640017700195315\n",
      "54.71051788330078\n",
      "26.475701904296876\n",
      "46.159088134765625\n",
      "49.15302022298177\n",
      "66.8128182547433\n",
      "84.60193379720052\n",
      "38.160868326822914\n",
      "50.66724141438802\n",
      "52.046244303385414\n",
      "37.329995727539064\n",
      "20.53452410016741\n",
      "31.838956197102863\n",
      "42.17073059082031\n",
      "63.11253051757812\n",
      "69.06246185302734\n",
      "42.57438659667969\n",
      "41.650033133370535\n",
      "42.26680162217882\n",
      "24.653512137276785\n",
      "40.868829345703126\n",
      "31.17901865641276\n",
      "37.6739247639974\n",
      "46.94205583844866\n",
      "55.58522687639509\n",
      "30.570780436197918\n",
      "44.40029035295759\n",
      "49.02313741048177\n",
      "50.93433634440104\n",
      "38.45894077845982\n",
      "33.55299072265625\n",
      "62.91929626464844\n",
      "54.59217529296875\n",
      "62.50486450195312\n",
      "22.900371551513672\n",
      "34.34137725830078\n",
      "19.435302734375\n",
      "59.3594482421875\n",
      "55.129996163504465\n",
      "26.114536830357142\n",
      "80.97391764322917\n",
      "45.019345092773435\n",
      "23.85517272949219\n",
      "46.259591238839285\n",
      "45.58848353794643\n",
      "56.709316677517364\n",
      "56.16846172626202\n",
      "70.88126220703126\n",
      "45.00146484375\n",
      "25.09646987915039\n",
      "37.136749267578125\n",
      "42.17422739664713\n",
      "50.890877859933035\n",
      "35.23204912458147\n",
      "58.408108181423614\n",
      "51.11391906738281\n",
      "44.054840087890625\n",
      "32.4107666015625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-307-c5b00f48e7d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mencoder1_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdecoder1_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder1_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder1_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-306-3786724f9005>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(encoder, decoder, encoder_optimizer, decoder_optimizer, epochs)\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[1;31m#Set the target as input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mdecoder_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Teacher forcing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gradient has to be a Tensor, Variable or None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder1 = EncoderRNN(vocab_size, hidden_size)\n",
    "decoder1 = DecoderRNN(hidden_size, vocab_size)\n",
    "encoder1_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate)\n",
    "decoder1_optimizer = optim.SGD(decoder1.parameters(), lr=learning_rate)\n",
    "training(encoder1, decoder1, encoder1_optimizer, decoder1_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=10):\n",
    "    input_variable = Variable(torch.LongTensor(sentence).view(-1, 1))\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    #encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        #encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[0]]))  # SOS\n",
    "    #decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    #decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == 1:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(ger_ix2vocab[ni])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        #decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    print(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1696, 1488, 2510, 1367, 161, 2, 1544, 191]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick brown fox jumped over the lazy dog'"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_english[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tom', 'frau', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht']\n"
     ]
    }
   ],
   "source": [
    "evaluate(encoder1, decoder1, test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
