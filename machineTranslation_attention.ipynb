{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine translation is a challenge for computers not to only understand human languages but also to generate languages. A machine translation can be viewed as a conditional language model, given a source sentence $x_i$, we needed to calculate the probability of generated sentence $p(y_i|x_i)$. In early years, statistical machine translation(SMT) was a focus, amongst which IBM models were basis, if you are interested, please visit Michael Collins' [webpage](http://www.cs.columbia.edu/~mcollins/), there he provided many useful and explicit lecture notes to illustrate basis terms and models of SMT.\n",
    "\n",
    "In recent years, with the development of artificial neural networks as well as deep learning applications, neural translation models were explored, especially [seq2seq](https://arxiv.org/pdf/1406.1078v3.pdf) model as well as later models has improved performances of machine translation.\n",
    "\n",
    "In the last part, we used a simple encoder-decoder model to translate English to German. In fact, since 2015, more efficient models such as attention models have been proposed and tested effectively in machine translation. In an attention model, the decoder doesn't rely on a fixed vector output by the encoder, but on a context vector varies according to the alignments of source and target sentences, and the context vector is a sum of product of attention weights and source sentence vectors. It is reasonable because different parts source sentences have different influence on target sentences. For more information, please read Dzmitry Bahdanau's paper [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) .\n",
    "\n",
    "Actually, there are some [excellent blogs](http://blog.csdn.net/u011414416/article/details/51057789) in Chinese, which introduced the development and theoretic models of neural machine translation explicitly and systemamtically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the paths of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Parameters\n",
    "data_dir = 'temp'\n",
    "data_file = 'eng_ger.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make data directory\n",
    "import os\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquiry\n",
    "\n",
    "Download the data from website if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class dataReader:\n",
    "    '''\n",
    "    Read text files from local drive.\n",
    "    If not exists, download it.\n",
    "    '''\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        if not self.__checkExists():\n",
    "            self.__download()\n",
    "        \n",
    "            \n",
    "    def loadData(self):\n",
    "        #print('Data Exists!')\n",
    "        eng_ger_data = []\n",
    "        with open(self.file_path, 'r') as in_conn:\n",
    "            for row in in_conn:\n",
    "                eng_ger_data.append(row[:-1])\n",
    "        return eng_ger_data\n",
    "\n",
    "    def __download(self):\n",
    "        '''Download text files which contain translation pairs'''\n",
    "        print('Data not found, downloading Eng-Ger sentences from www.manythings.org')\n",
    "        sentence_url = 'http://www.manythings.org/anki/deu-eng.zip'\n",
    "        r = urllib.request.urlopen(sentence_url)\n",
    "        z = ZipFile(io.BytesIO(r.read()))\n",
    "        file = z.read('deu.txt')\n",
    "        # Format Data\n",
    "        eng_ger_data = file.decode()\n",
    "        eng_ger_data = eng_ger_data.encode('ascii',errors='ignore')\n",
    "        eng_ger_data = eng_ger_data.decode().split('\\n')\n",
    "        # Write to file\n",
    "        with open(self.file_path, 'w') as out_conn:\n",
    "            for sentence in eng_ger_data:\n",
    "                out_conn.write(sentence + '\\n')\n",
    "                \n",
    "    def __checkExists(self):\n",
    "        '''Check the file'''\n",
    "        return os.path.isfile(self.file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dr = dataReader('eng_ger.txt')\n",
    "eng_ger_data = dr.loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi.\\tHallo!',\n",
       " 'Hi.\\tGr Gott!',\n",
       " 'Run!\\tLauf!',\n",
       " 'Wow!\\tPotzdonner!',\n",
       " 'Wow!\\tDonnerwetter!',\n",
       " 'Fire!\\tFeuer!',\n",
       " 'Help!\\tHilfe!',\n",
       " 'Help!\\tZu Hlf!',\n",
       " 'Stop!\\tStopp!',\n",
       " 'Wait!\\tWarte!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_ger_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processig\n",
    "\n",
    "Preprocess the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "vocab_size = 10000\n",
    "class textHandler:\n",
    "    '''Split sentences into pairs of Source-Target language'''\n",
    "    def __init__(self, data, vocab_size):\n",
    "        self.data = data\n",
    "        self.vocab_size = vocab_size\n",
    "        self.__sentSplit()\n",
    "        \n",
    "    def __removePunctuation(self):\n",
    "        '''Remove punctuation'''\n",
    "        # Remove punctuation\n",
    "        punct = string.punctuation\n",
    "        pair_data = [''.join(char for char in sent if char not in punct) for sent in self.data]\n",
    "        return pair_data\n",
    "    \n",
    "    def __sentSplit(self):\n",
    "        # Break each sentence pair by tabs, one part is English, the other is German. \n",
    "        pair_data = self.__removePunctuation()\n",
    "        s_t_data = [x.split('\\t') for x in pair_data if len(x)>=1]\n",
    "        [source_sentence, target_sentence] = [list(x) for x in zip(*s_t_data)]\n",
    "        #Split each sentence into words\n",
    "        self.source_sentence = [x.lower().split() for x in source_sentence]\n",
    "        self.target_sentence = [x.lower().split() for x in target_sentence]\n",
    "        #return source_sentence, target_sentence\n",
    "    \n",
    "    def __buildVocab(self, sents):\n",
    "        '''Build Vocabulary for both languages'''\n",
    "        # Process the English Vocabulary\n",
    "        all_words = [word for sent in sents for word in sent]\n",
    "        #Count the frequency of English words\n",
    "        all_words_counts = Counter(all_words)\n",
    "        #Get the most frequent vocab_size words, left regarded as unknow\n",
    "        word_keys = [x[0] for x in all_words_counts.most_common(self.vocab_size-3)] \n",
    "        #Word to ID, set Starting token as 'SOS', ending token as 'EOS'\n",
    "        vocab2ix = dict(zip(word_keys, range(2,self.vocab_size-1)))\n",
    "        vocab2ix['SOS'] = 0\n",
    "        vocab2ix['EOS'] = 1\n",
    "        #ID to Word\n",
    "        ix2vocab = {val:key for key, val in vocab2ix.items()}\n",
    "        return vocab2ix, ix2vocab\n",
    "    \n",
    "    def getSents(self):\n",
    "        '''Get preprocessed sentences'''\n",
    "        return self.source_sentence, self.target_sentence\n",
    "    \n",
    "    def sent2vec(self, sents, vocab2ix):\n",
    "        '''Transform sentences into Ids'''\n",
    "        processed = []\n",
    "        for sent in sents:\n",
    "            temp_sentence = []\n",
    "            for word in sent:\n",
    "                try:\n",
    "                    temp_sentence.append(vocab2ix[word])\n",
    "                except:\n",
    "                    #Unknown words\n",
    "                    temp_sentence.append(self.vocab_size-1)\n",
    "            processed.append(temp_sentence)\n",
    "        return processed\n",
    "    \n",
    "    def generateVocab(self):\n",
    "        '''Generate Vocabulary'''\n",
    "        #source_sentence, target_sentence = self.__sentSplit()\n",
    "        source_vocab2ix, source_ix2vocab = self.__buildVocab(self.source_sentence)\n",
    "        target_vocab2ix, target_ix2vocab = self.__buildVocab(self.target_sentence)\n",
    "        return source_vocab2ix, source_ix2vocab, target_vocab2ix, target_ix2vocab\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "th = textHandler(data=eng_ger_data, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_sentence, german_sentence = th.getSents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eng_vocab2ix, eng_ix2vocab, ger_vocab2ix, ger_ix2vocab = th.generateVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_processed = th.sent2vec(english_sentence, eng_vocab2ix)\n",
    "german_processed = th.sent2vec(german_sentence, ger_vocab2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = ['I love this dog', 'What a nice day', 'This is a book']\n",
    "test_data = [x.lower().split() for x in test_data]\n",
    "test_data = th.sent2vec(test_data, eng_vocab2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 168, 17, 191], [24, 7, 392, 117], [17, 8, 7, 123]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Build a simple encoder-decoder architecture\n",
    "\n",
    "In this demo, we use a simple encoder-decoder architecture to train and infer translations. We encode all the word vectors in source sentences into a fixed vector, then make the fixed vector as an input for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #Get embedding series of input words\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        #Compress the input vectors into RNN\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        #Create a initial zero hidden state\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        result = result.cuda() if use_cuda else result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=10):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_output, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "In order to understand the mechanism of neural machine translation, we wrap a pair of translation sentences each time instead of a batch of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "max_length = 10\n",
    "encoder1 = EncoderRNN(vocab_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 100\n",
    "input_variable, target_variable = english_processed[index], german_processed[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform the input data and target into Variable vectors\n",
    "input_variable = Variable(torch.LongTensor(input_variable).view(-1, 1))\n",
    "target_variable = Variable(torch.LongTensor(target_variable).view(-1, 1))\n",
    "input_length = input_variable.size()[0]\n",
    "target_length = target_variable.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "We can compress a sereis of word embeddings into a final hidden state and output through RNN.\n",
    "$$h_t = f(h_{t-1}, x_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_hidden = encoder1.initHidden()\n",
    "encoder_outputs = Variable(torch.zeros(max_length, encoder1.hidden_size))\n",
    "#Calculate the final state of input words\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder1(\n",
    "        input_variable[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder with Attention\n",
    "\n",
    "In the decoder part,for training, we only take two inputs into consideration: The first is the target variables provided, and the second is the previous hidden state initialized by the final state($C_T$) of the encoder.\n",
    "$$h_t = f(h_{t-1}, y_{t-1}), h_0=C_T$$\n",
    "\n",
    "The architecture is as below(quoted from http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html):\n",
    "![encoder-decoder](https://i.imgur.com/1152PYf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an instance for decoder\n",
    "decoder1 = AttnDecoderRNN(hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder1.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = 0\n",
    "criterion = nn.NLLLoss()\n",
    "decoder_input = Variable(torch.LongTensor([[0]]))\n",
    "#Set the beginning hidden state of decoder as the final state of encoder\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(target_length):\n",
    "    decoder_output, decoder_hidden, decoder_attention = decoder1(\n",
    "        decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "    loss += criterion(decoder_output, target_variable[di])\n",
    "    decoder_input = target_variable[di]\n",
    "    #decoder_output, decoder_hidden = decoder1(\n",
    "        #decoder_input, decoder_hidden)\n",
    "    #loss += criterion(decoder_output[0], target_variable[di])\n",
    "    #Set the target as input\n",
    "    #decoder_input = target_variable[di]  # Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 18.3376\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a, b = decoder_output.data.topk(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-8.7031 -8.7401\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 9335  9001\n",
       "[torch.LongTensor of size 1x2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap it up\n",
    "\n",
    "Now, we can put the training procedures in one function. Note, in the paper [Sequence to Sequence Learning with Neural Networks\n",
    "](https://arxiv.org/pdf/1409.3215.pdf), they mentioned that it would be more efficient if we reversed the order of words in source sentences because the encoder could retain more information of the last few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "compressed = list(zip(english_processed, german_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_pairs = copy.deepcopy(compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def appendTokens(s):\n",
    "    '''Add ending tokens to each sentence'''\n",
    "    s.append(1)\n",
    "    return s\n",
    "\n",
    "def reverseSent(sent):\n",
    "    '''Reverse the order of words in a sentence'''\n",
    "    sent = list(reversed(sent))\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Filter those long sentences\n",
    "pairs_filtered = []\n",
    "#Because we need to add one ending tokens later, so substract 1 here\n",
    "for item in sent_pairs:\n",
    "    if len(item[0]) <= (max_length-1) and len(item[0]) > 3:\n",
    "        s, t = item[0], item[1]\n",
    "        s = reverseSent(s)\n",
    "        pairs_filtered.append((s, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "criterion = nn.NLLLoss()\n",
    "def training(encoder, decoder, encoder_optimizer, decoder_optimizer, epochs=1):\n",
    "    for e in range(epochs):\n",
    "        np.random.shuffle(pairs_filtered)\n",
    "        for c, pair in enumerate(pairs_filtered):\n",
    "            #Add ending tokens for each pair\n",
    "            input_data, target_data = pair[0], pair[1]\n",
    "            input_data.append(1)\n",
    "            target_data.append(1)\n",
    "            #Transform the input data and target into Variable vectors\n",
    "            input_variable = Variable(torch.LongTensor(input_data).view(-1, 1))\n",
    "            target_variable = Variable(torch.LongTensor(target_data).view(-1, 1))\n",
    "            input_length = input_variable.size()[0]\n",
    "            target_length = target_variable.size()[0]\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "            #Calculate the final state of input words\n",
    "            for i in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(\n",
    "                    input_variable[i], encoder_hidden)\n",
    "                if i == max_length:\n",
    "                    print(c, pair)\n",
    "                encoder_outputs[i] = encoder_output[0][0]\n",
    "            #Clear grads\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            decoder_input = Variable(torch.LongTensor([[0]]))\n",
    "            #Set the beginning hidden state of decoder as the final state of encoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "                #print(decoder_output[0].size())\n",
    "                #print('*'*20)\n",
    "                #print(target_variable[di])\n",
    "                loss += criterion(decoder_output[0], target_variable[di])\n",
    "                #Set the target as input\n",
    "                decoder_input = target_variable[di]  # Teacher forcing\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            if c%200 == 0:\n",
    "                print(loss.data[0] / target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.170216151646205\n",
      "7.783127466837565\n",
      "6.7661387125651045\n",
      "7.108184051513672\n",
      "7.126961602105035\n",
      "6.537042617797852\n",
      "7.073707580566406\n",
      "7.209368619051847\n",
      "4.293683460780552\n",
      "3.0778300762176514\n",
      "7.693224589029948\n",
      "5.133889675140381\n",
      "5.284617900848389\n",
      "5.666646321614583\n",
      "6.848593235015869\n",
      "4.645271846226284\n",
      "7.463876247406006\n",
      "6.771103541056315\n",
      "6.095650355021159\n",
      "6.110006159002131\n",
      "5.99597396850586\n",
      "7.005320739746094\n",
      "6.046659469604492\n",
      "6.930378723144531\n",
      "5.542941093444824\n",
      "6.229775565011161\n",
      "4.443459647042411\n",
      "4.175717671712239\n",
      "5.080857849121093\n",
      "6.138928549630301\n",
      "6.7208396911621096\n",
      "4.7760311762491865\n",
      "4.5302934646606445\n",
      "5.5197834968566895\n",
      "5.692255292619977\n",
      "5.498741531372071\n",
      "5.230793635050456\n",
      "5.74639892578125\n",
      "5.448177337646484\n",
      "5.012805302937825\n",
      "6.188316345214844\n",
      "7.247036457061768\n",
      "5.485659463065011\n",
      "4.322620868682861\n",
      "5.635908762613933\n",
      "6.133015223911831\n",
      "6.197908401489258\n",
      "5.494338353474935\n",
      "4.8275861740112305\n",
      "4.98302502102322\n",
      "4.863011360168457\n",
      "5.294653756277902\n",
      "4.9899444580078125\n",
      "5.000674353705512\n",
      "6.345096934925426\n",
      "7.045615059988839\n",
      "6.013236363728841\n",
      "5.497699064366958\n",
      "3.470198392868042\n",
      "5.392355600992839\n",
      "6.031267801920573\n",
      "4.763840357462565\n",
      "4.154368209838867\n",
      "6.2932084401448565\n",
      "5.930233383178711\n",
      "5.065974644252232\n",
      "5.104644775390625\n",
      "4.448809051513672\n",
      "5.128107070922852\n",
      "6.287438201904297\n",
      "5.497524261474609\n",
      "5.888740062713623\n",
      "5.871105575561524\n",
      "4.465056828090122\n",
      "5.414721298217773\n",
      "3.547341156005859\n",
      "4.112542046440972\n",
      "3.7531725565592446\n",
      "4.6506451198032925\n",
      "4.875365257263184\n",
      "4.78541510445731\n",
      "4.393406731741769\n",
      "6.159356435139974\n",
      "4.865185101826985\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-c5b00f48e7d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mencoder1_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdecoder1_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder1_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder1_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-ab3c89d47d75>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(encoder, decoder, encoder_optimizer, decoder_optimizer, epochs)\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[1;31m#Set the target as input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mdecoder_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Teacher forcing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\myPytorch\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gradient has to be a Tensor, Variable or None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder1 = EncoderRNN(vocab_size, hidden_size)\n",
    "decoder1 = DecoderRNN(hidden_size, vocab_size)\n",
    "encoder1_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate)\n",
    "decoder1_optimizer = optim.SGD(decoder1.parameters(), lr=learning_rate)\n",
    "training(encoder1, decoder1, encoder1_optimizer, decoder1_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And next, we use a greedy method to generate a target sentence based on source sentence. Each time, we selected the word wich has the maximum probability untile the decoder generate an ending token 'EOS'. However, a best choice each time does not guarantte the most-likely sentence in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=10):\n",
    "    input_variable = Variable(torch.LongTensor(sentence).view(-1, 1))\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    #encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        #encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "    \n",
    "    #Set the inital value as SOS token\n",
    "    decoder_input = Variable(torch.LongTensor([[0]]))  # SOS\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoded_words = []\n",
    "\n",
    "    #Greedy method\n",
    "    #for di in range(max_length):\n",
    "        #decoder_output, decoder_hidden = decoder(\n",
    "            #decoder_input, decoder_hidden)\n",
    "        #top_value, top_index = decoder_output.data.topk(1)\n",
    "        #Get the index of the word\n",
    "        #ni = top_index[0][0]\n",
    "        #if ni == 1:\n",
    "            #decoded_words.append('<EOS>')\n",
    "            #break\n",
    "        #else:\n",
    "            ##decoded_words.append(ger_ix2vocab[ni])\n",
    "\n",
    "        #decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        #decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    \n",
    "    #Beam search method\n",
    "    #Get the largest k values and their indice\n",
    "    k = 2\n",
    "    top_value, top_index, decoder_hidden = beamsearch_topk(decoder, decoder_input, decoder_hidden)\n",
    "    log_sum = top_value[0]\n",
    "    current_index = top_index[0]\n",
    "    #Create a recorder to record path\n",
    "    record_path_dict = {}\n",
    "    for i in range(k):\n",
    "        record_path_dict[i] = []\n",
    "    #recordd hidden state\n",
    "    decoder_hidden_dict = {}\n",
    "    for i in range(k):\n",
    "        decoder_hidden_dict[i] = decoder_hidden\n",
    "    for di in range(1, max_length):\n",
    "        previous_index = current_index\n",
    "        #Traverse each selected word\n",
    "        log_sum_list = []\n",
    "        k_top_list = []\n",
    "        for i, ix in enumerate(previous_index):\n",
    "            input_id = Variable(torch.LongTensor([[ix]]))\n",
    "            top_value_temp, top_index_temp, decoder_hidden_temp = beamsearch_topk(decoder, \n",
    "                                                                                  input_id, decoder_hidden_dict[i])\n",
    "            for j, v in enumerate(top_value_temp[0]):\n",
    "                temp_sum = log_sum[i] + v\n",
    "                #Record sums\n",
    "                log_sum_list.append(temp_sum)\n",
    "                #Record pairs\n",
    "                k_top_list.append([ix, top_index_temp[0][j], decoder_hidden_temp])\n",
    "                \n",
    "        sum_ix_pair = list(zip(log_sum_list, k_top_list))\n",
    "        sum_ix_pair.sort(reverse=True)\n",
    "        for i in range(k):\n",
    "            #Update selected word ID and log sums of probability\n",
    "            current_index[i] = sum_ix_pair[i][1][1]\n",
    "            log_sum[i] += sum_ix_pair[i][0]\n",
    "            #Record the path\n",
    "            record_path_dict[i].append(sum_ix_pair[i][1][:2])\n",
    "            decoder_hidden_dict[i] = sum_ix_pair[i][1][2]\n",
    "\n",
    "        #first, second = top_index[0][0], top_index[0][1]\n",
    "        #if first == 1:\n",
    "            #decoded_words.append('<EOS>')\n",
    "            #break\n",
    "        #else:\n",
    "            #decoded_words.append(ger_ix2vocab[first])\n",
    "\n",
    "        #decoder_input = Variable(torch.LongTensor([[first]]))\n",
    "\n",
    "    print(record_path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beamsearch_topk(decoder, decoder_input, decoder_hidden):\n",
    "    decoder_output, decoder_hidden = decoder(\n",
    "        decoder_input, decoder_hidden)\n",
    "    top_value, top_index = decoder_output.data.topk(2)\n",
    "    #first, second = top_index[0][0], top_index[0][1]\n",
    "    return top_value, top_index, decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 7, 392, 117]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [[12, 4], [4, 9999], [9999, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]], 1: [[8, 4], [4, 16], [16, 9999], [9999, 1], [1, 9], [1, 9], [1, 9], [1, 9], [1, 9]]}\n"
     ]
    }
   ],
   "source": [
    "evaluate(encoder1, decoder1, test_data[2])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:myPytorch]",
   "language": "python",
   "name": "conda-env-myPytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
