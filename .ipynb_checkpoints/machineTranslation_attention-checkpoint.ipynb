{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine translation is a challenge for computers not to only understand human languages but also to generate languages. A machine translation can be viewed as a conditional language model, given a source sentence $x_i$, we needed to calculate the probability of generated sentence $p(y_i|x_i)$. In early years, statistical machine translation(SMT) was a focus, amongst which IBM models were basis, if you are interested, please visit Michael Collins' [webpage](http://www.cs.columbia.edu/~mcollins/), there he provided many useful and explicit lecture notes to illustrate basis terms and models of SMT.\n",
    "\n",
    "In recent years, with the development of artificial neural networks as well as deep learning applications, neural translation models were explored, especially [seq2seq](https://arxiv.org/pdf/1406.1078v3.pdf) model as well as later models has improved performances of machine translation.\n",
    "\n",
    "In the last part, we used a simple encoder-decoder model to translate English to German. In fact, since 2015, more efficient models such as attention models have been proposed and tested effectively in machine translation. In an attention model, the decoder doesn't rely on a fixed vector output by the encoder, but on a context vector varies according to the alignments of source and target sentences, and the context vector is a sum of product of attention weights and source sentence vectors. It is reasonable because different parts source sentences have different influence on target sentences. For more information, please read Dzmitry Bahdanau's paper [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) .\n",
    "\n",
    "Actually, there are some [excellent blogs](http://blog.csdn.net/u011414416/article/details/51057789) in Chinese, which introduced the development and theoretic models of neural machine translation explicitly and systemamtically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the paths of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Parameters\n",
    "data_dir = 'temp'\n",
    "data_file = 'eng_ger.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make data directory\n",
    "import os\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquiry\n",
    "\n",
    "Download the data from website if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class dataReader:\n",
    "    '''\n",
    "    Read text files from local drive.\n",
    "    If not exists, download it.\n",
    "    '''\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        if not self.__checkExists():\n",
    "            self.__download()\n",
    "        \n",
    "            \n",
    "    def loadData(self):\n",
    "        #print('Data Exists!')\n",
    "        eng_ger_data = []\n",
    "        with open(self.file_path, 'r') as in_conn:\n",
    "            for row in in_conn:\n",
    "                eng_ger_data.append(row[:-1])\n",
    "        return eng_ger_data\n",
    "\n",
    "    def __download(self):\n",
    "        '''Download text files which contain translation pairs'''\n",
    "        print('Data not found, downloading Eng-Ger sentences from www.manythings.org')\n",
    "        sentence_url = 'http://www.manythings.org/anki/deu-eng.zip'\n",
    "        r = urllib.request.urlopen(sentence_url)\n",
    "        z = ZipFile(io.BytesIO(r.read()))\n",
    "        file = z.read('deu.txt')\n",
    "        # Format Data\n",
    "        eng_ger_data = file.decode()\n",
    "        eng_ger_data = eng_ger_data.encode('ascii',errors='ignore')\n",
    "        eng_ger_data = eng_ger_data.decode().split('\\n')\n",
    "        # Write to file\n",
    "        with open(self.file_path, 'w') as out_conn:\n",
    "            for sentence in eng_ger_data:\n",
    "                out_conn.write(sentence + '\\n')\n",
    "                \n",
    "    def __checkExists(self):\n",
    "        '''Check the file'''\n",
    "        return os.path.isfile(self.file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dr = dataReader('eng_ger.txt')\n",
    "eng_ger_data = dr.loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi.\\tHallo!',\n",
       " 'Hi.\\tGr Gott!',\n",
       " 'Run!\\tLauf!',\n",
       " 'Wow!\\tPotzdonner!',\n",
       " 'Wow!\\tDonnerwetter!',\n",
       " 'Fire!\\tFeuer!',\n",
       " 'Help!\\tHilfe!',\n",
       " 'Help!\\tZu Hlf!',\n",
       " 'Stop!\\tStopp!',\n",
       " 'Wait!\\tWarte!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_ger_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processig\n",
    "\n",
    "Preprocess the original data. We can define a class to remove punctuation, split the original sentences, then build corresponding vocabulary for source language and target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "vocab_size = 10000\n",
    "class textHandler:\n",
    "    '''Split sentences into pairs of Source-Target language'''\n",
    "    def __init__(self, data, vocab_size):\n",
    "        self.data = data\n",
    "        self.vocab_size = vocab_size\n",
    "        self.__sentSplit()\n",
    "        \n",
    "    def __removePunctuation(self):\n",
    "        '''Remove punctuation'''\n",
    "        # Remove punctuation\n",
    "        punct = string.punctuation\n",
    "        pair_data = [''.join(char for char in sent if char not in punct) for sent in self.data]\n",
    "        return pair_data\n",
    "    \n",
    "    def __sentSplit(self):\n",
    "        # Break each sentence pair by tabs, one part is English, the other is German. \n",
    "        pair_data = self.__removePunctuation()\n",
    "        s_t_data = [x.split('\\t') for x in pair_data if len(x)>=1]\n",
    "        [source_sentence, target_sentence] = [list(x) for x in zip(*s_t_data)]\n",
    "        #Split each sentence into words\n",
    "        self.source_sentence = [x.lower().split() for x in source_sentence]\n",
    "        self.target_sentence = [x.lower().split() for x in target_sentence]\n",
    "        #return source_sentence, target_sentence\n",
    "    \n",
    "    def __buildVocab(self, sents):\n",
    "        '''Build Vocabulary for both languages'''\n",
    "        # Process the English Vocabulary\n",
    "        all_words = [word for sent in sents for word in sent]\n",
    "        #Count the frequency of English words\n",
    "        all_words_counts = Counter(all_words)\n",
    "        #Get the most frequent vocab_size words, left regarded as unknow\n",
    "        word_keys = [x[0] for x in all_words_counts.most_common(self.vocab_size-3)] \n",
    "        #Word to ID, set Starting token as 'SOS', ending token as 'EOS'\n",
    "        vocab2ix = dict(zip(word_keys, range(2,self.vocab_size)))\n",
    "        vocab2ix['SOS'] = 0\n",
    "        vocab2ix['EOS'] = 1\n",
    "        vocab2ix['UNK'] = self.vocab_size - 1\n",
    "        #ID to Word\n",
    "        ix2vocab = {val:key for key, val in vocab2ix.items()}\n",
    "        return vocab2ix, ix2vocab\n",
    "    \n",
    "    def getSents(self):\n",
    "        '''Get preprocessed sentences'''\n",
    "        return self.source_sentence, self.target_sentence\n",
    "    \n",
    "    def sent2vec(self, sents, vocab2ix):\n",
    "        '''Transform sentences into Ids'''\n",
    "        processed = []\n",
    "        for sent in sents:\n",
    "            temp_sentence = []\n",
    "            for word in sent:\n",
    "                try:\n",
    "                    temp_sentence.append(vocab2ix[word])\n",
    "                except:\n",
    "                    #Unknown words\n",
    "                    temp_sentence.append(self.vocab_size-1)\n",
    "            processed.append(temp_sentence)\n",
    "        return processed\n",
    "    \n",
    "    def generateVocab(self):\n",
    "        '''Generate Vocabulary'''\n",
    "        #source_sentence, target_sentence = self.__sentSplit()\n",
    "        source_vocab2ix, source_ix2vocab = self.__buildVocab(self.source_sentence)\n",
    "        target_vocab2ix, target_ix2vocab = self.__buildVocab(self.target_sentence)\n",
    "        return source_vocab2ix, source_ix2vocab, target_vocab2ix, target_ix2vocab\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "th = textHandler(data=eng_ger_data, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split sentences into tokens\n",
    "english_sentence, german_sentence = th.getSents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get vocabulary\n",
    "eng_vocab2ix, eng_ix2vocab, ger_vocab2ix, ger_ix2vocab = th.generateVocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have encoded each word(including starting, ending tokens and unknown ones) as an ID. We can transform texts into sequences of numbers before we feed them into algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transform tokens into IDs\n",
    "english_processed = th.sent2vec(english_sentence, eng_vocab2ix)\n",
    "german_processed = th.sent2vec(german_sentence, ger_vocab2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = ['I love this dog', 'What a nice day', 'This is a book']\n",
    "test_data = [x.lower().split() for x in test_data]\n",
    "test_data = th.sent2vec(test_data, eng_vocab2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 168, 17, 191], [24, 7, 392, 117], [17, 8, 7, 123]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Build an encoder-decoder architecture\n",
    "\n",
    "In this demo, we use a simple encoder-decoder architecture to train and infer translations. Unlike a simple encoder-decoder system, we do not compress all the source sentence into a fixed vector as an input for the decoder. Instead, we use an attention mechanism to represent source sentences dynamically for different source-target pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru = nn.GRU(hidden_size, hidden_size, bidirectional=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on GRU in module torch.nn.modules.rnn object:\n",
      "\n",
      "class GRU(RNNBase)\n",
      " |  Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n",
      " |  \n",
      " |  \n",
      " |  For each element in the input sequence, each layer computes the following\n",
      " |  function:\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |          \\begin{array}{ll}\n",
      " |          r_t = sigmoid(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n",
      " |          i_t = sigmoid(W_{ii} x_t + b_{ii} + W_hi h_{(t-1)} + b_{hi}) \\\\\n",
      " |          n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n",
      " |          h_t = (1 - i_t) * n_t + i_t * h_{(t-1)} \\\\\n",
      " |          \\end{array}\n",
      " |  \n",
      " |  where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is the hidden\n",
      " |  state of the previous layer at time `t` or :math:`input_t` for the first layer,\n",
      " |  and :math:`r_t`, :math:`i_t`, :math:`n_t` are the reset, input, and new gates, respectively.\n",
      " |  \n",
      " |  Args:\n",
      " |      input_size: The number of expected features in the input x\n",
      " |      hidden_size: The number of features in the hidden state h\n",
      " |      num_layers: Number of recurrent layers.\n",
      " |      bias: If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
      " |      batch_first: If True, then the input and output tensors are provided as (batch, seq, feature)\n",
      " |      dropout: If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer\n",
      " |      bidirectional: If True, becomes a bidirectional RNN. Default: False\n",
      " |  \n",
      " |  Inputs: input, h_0\n",
      " |      - **input** (seq_len, batch, input_size): tensor containing the features of the input sequence.\n",
      " |        The input can also be a packed variable length sequence. See :func:`torch.nn.utils.rnn.pack_padded_sequence`\n",
      " |        for details.\n",
      " |      - **h_0** (num_layers * num_directions, batch, hidden_size): tensor containing the initial\n",
      " |        hidden state for each element in the batch.\n",
      " |  \n",
      " |  Outputs: output, h_n\n",
      " |      - **output** (seq_len, batch, hidden_size * num_directions): tensor containing the output features h_t from\n",
      " |        the last layer of the RNN, for each t. If a :class:`torch.nn.utils.rnn.PackedSequence` has been given as the\n",
      " |        input, the output will also be a packed sequence.\n",
      " |      - **h_n** (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t=seq_len\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight_ih_l[k] : the learnable input-hidden weights of the k-th layer (W_ir|W_ii|W_in), of shape\n",
      " |                       `(input_size x 3*hidden_size)`\n",
      " |      weight_hh_l[k] : the learnable hidden-hidden weights of the k-th layer (W_hr|W_hi|W_hn), of shape\n",
      " |                       `(hidden_size x 3*hidden_size)`\n",
      " |      bias_ih_l[k] : the learnable input-hidden bias of the k-th layer (b_ir|b_ii|b_in), of shape\n",
      " |                       `(3*hidden_size)`\n",
      " |      bias_hh_l[k] : the learnable hidden-hidden bias of the k-th layer (W_hr|W_hi|W_hn), of shape\n",
      " |                       `(3*hidden_size)`\n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> rnn = nn.GRU(10, 20, 2)\n",
      " |      >>> input = Variable(torch.randn(5, 3, 10))\n",
      " |      >>> h0 = Variable(torch.randn(2, 3, 20))\n",
      " |      >>> output, hn = rnn(input, h0)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GRU\n",
      " |      RNNBase\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RNNBase:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, d)\n",
      " |  \n",
      " |  forward(self, input, hx=None)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overriden by all subclasses.\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from RNNBase:\n",
      " |  \n",
      " |  all_weights\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |  \n",
      " |  cpu(self, device_id=None)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |  \n",
      " |  cuda(self, device_id=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device_id (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all parameters and buffers to double datatype.\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all parameters and buffers to float datatype.\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all parameters and buffers to half datatype.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. The keys of :attr:`state_dict` must\n",
      " |      exactly match the keys returned by this module's :func:`state_dict()`\n",
      " |      function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): A dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, memo=None)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time :func:`forward` computes an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='')\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        #Embedding for souce words\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        #Bidirectional RNN\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, bidirectional=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #Get embedding series of input words\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        #Compress the input vectors into RNN\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        #Create a initial zero hidden state\n",
    "        result = Variable(torch.zeros(2, 1, self.hidden_size))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For more information, please refer to https://arxiv.org/pdf/1409.0473.pdf\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    '''Use attention model to decode target sentences'''\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=10):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        #Use dropout to prevent against overfitting\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        #Embedding for target words\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        #Define functions to calculate attentions\n",
    "        self.attn = nn.Linear(self.hidden_size * 3, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_output, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        #Alignment factors, concatenate last hidden state of target\n",
    "        #And current source word vector\n",
    "        e = self.attn(torch.cat((embedded[0], hidden[0], hidden[0]), 1))\n",
    "        #e = self.attn(torch.cat((encoder_hidden[0], hidden[0]), 1))\n",
    "        #Calculate Attention weights based on alignment factors\n",
    "        attn_weights = F.softmax(e)\n",
    "        #Conext vectors derived by attention weights and \n",
    "        context_vec = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        #Concatenate the embedded and context vector\n",
    "        output = torch.cat((embedded[0], context_vec[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        #Decode target sentence\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note, this attention model is a little bit different from that proposed by Dzmitry Bahdanau, in his paper the attention weights were calculated by the target hidden state and source annotation sequences represented by bidirectional RNNs, whereas in this project the source annotation sequences were simply represented by word vectors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE denote the ith word of the source sentence as $h_i$, there are many ways to represent $h_i$, for example, we can simply use the embedding of ith word as $h_i$, or we can use RNN hidden state of the ith word as $h_i$, even a concatenation of bidirectional RNN hidden states to represent $h_i$.\n",
    "\n",
    "In an attention model, the decoder part's input consists of previous hidden state, previous target word and the context vector of the source sentence, we denote it as $c_j$(suppose the index of current target is j), and the attention weights as $\\alpha$. And $\\alpha$ can be derived from previous target hidden state and the source sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$c_j = \\sum_{i=1} \\alpha_{ji} h_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "In order to understand the mechanism of neural machine translation, we wrap a pair of translation sentences each time instead of a batch of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "max_length = 10\n",
    "encoder1 = EncoderRNN(vocab_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 100\n",
    "input_variable, target_variable = english_processed[index], german_processed[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform the input data and target into Variable vectors\n",
    "input_variable = Variable(torch.LongTensor(input_variable).view(-1, 1))\n",
    "target_variable = Variable(torch.LongTensor(target_variable).view(-1, 1))\n",
    "input_length = input_variable.size()[0]\n",
    "target_length = target_variable.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "We can compress a sereis of word embeddings into a final hidden state and output through RNN. Note we use Bidirectional-RNN here, so the output should contain two hidden states.\n",
    "$$h_t = f(h_{t-1}, x_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_hidden = encoder1.initHidden()\n",
    "encoder_outputs = Variable(torch.zeros(max_length, 2*encoder1.hidden_size))\n",
    "#Calculate the final state of input words\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder1(\n",
    "        input_variable[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder with Attention\n",
    "\n",
    "In the decoder part,for training, we only take two inputs into consideration: The first is the target variables provided, and the second is the previous hidden state initialized by the final state($C_T$) of the encoder.\n",
    "$$h_t = f(h_{t-1}, y_{t-1}), h_0=C_T$$\n",
    "\n",
    "The architecture is as below(quoted from http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html):\n",
    "![encoder-decoder](https://i.imgur.com/1152PYf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an instance for decoder\n",
    "decoder1 = AttnDecoderRNN(hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder1.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = 0\n",
    "criterion = nn.NLLLoss()\n",
    "decoder_input = Variable(torch.LongTensor([[0]]))\n",
    "#Set the beginning hidden state of decoder as the final state of encoder\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(target_length):\n",
    "    decoder_output, decoder_hidden, decoder_attention = decoder1(\n",
    "        decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "    loss += criterion(decoder_output, target_variable[di])\n",
    "    decoder_input = target_variable[di]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 18.3926\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap it up\n",
    "\n",
    "Now, we can put the training procedures in one function. Note, in the paper [Sequence to Sequence Learning with Neural Networks\n",
    "](https://arxiv.org/pdf/1409.3215.pdf), they mentioned that it would be more efficient if we reversed the order of words in source sentences because the encoder could retain more information of the last few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "compressed = list(zip(english_processed, german_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_pairs = copy.deepcopy(compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Filter those long sentences\n",
    "pairs_filtered = []\n",
    "#Because we need to add one ending tokens later, so substract 1 here\n",
    "for item in sent_pairs:\n",
    "    if len(item[0]) <= (max_length-1) and len(item[0]) > 3:\n",
    "        pairs_filtered.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "criterion = nn.NLLLoss()\n",
    "def training(encoder, decoder, encoder_optimizer, decoder_optimizer, epochs=1):\n",
    "    for e in range(epochs):\n",
    "        np.random.shuffle(pairs_filtered)\n",
    "        for c, pair in enumerate(pairs_filtered):\n",
    "            #Add ending tokens for each pair\n",
    "            input_data, target_data = pair[0], pair[1]\n",
    "            input_data.append(1)\n",
    "            target_data.append(1)\n",
    "            #Transform the input data and target into Variable vectors\n",
    "            input_variable = Variable(torch.LongTensor(input_data).view(-1, 1))\n",
    "            target_variable = Variable(torch.LongTensor(target_data).view(-1, 1))\n",
    "            input_length = input_variable.size()[0]\n",
    "            target_length = target_variable.size()[0]\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "            #Calculate the final state of input words\n",
    "            for i in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(\n",
    "                    input_variable[i], encoder_hidden)\n",
    "                encoder_outputs[i] = encoder_output[0][0]\n",
    "            #Clear grads\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            decoder_input = Variable(torch.LongTensor([[0]]))\n",
    "            #Set the beginning hidden state of decoder as the final state of encoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "                loss += criterion(decoder_output[0], target_variable[di])\n",
    "                decoder_input = target_variable[di]\n",
    "                #print(decoder_output[0].size())\n",
    "                #print('*'*20)\n",
    "                #print(target_variable[di])\n",
    "                #loss += criterion(decoder_output[0], target_variable[di])\n",
    "                #Set the target as input\n",
    "                #decoder_input = target_variable[di]  # Teacher forcing\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            if c%200 == 0:\n",
    "                print(loss.data[0] / target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor size at d:\\downloads\\pytorch-master-1\\torch\\lib\\th\\generic/THTensorCopy.c:51",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-d10028068e04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mencoder1_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdecoder1_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder1_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder1_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-97-82a5bb27cbce>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(encoder, decoder, encoder_optimizer, decoder_optimizer, epochs)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 encoder_output, encoder_hidden = encoder(\n\u001b[0;32m     21\u001b[0m                     input_variable[i], encoder_hidden)\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;31m#Clear grads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\myPytorch\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mMaskedFill\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mSetItem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__deepcopy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\myPytorch\\lib\\site-packages\\torch\\autograd\\_functions\\tensor.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, i, index, value)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: inconsistent tensor size at d:\\downloads\\pytorch-master-1\\torch\\lib\\th\\generic/THTensorCopy.c:51"
     ]
    }
   ],
   "source": [
    "encoder1 = EncoderRNN(vocab_size, hidden_size)\n",
    "decoder1 = AttnDecoderRNN(hidden_size, vocab_size)\n",
    "encoder1_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate)\n",
    "decoder1_optimizer = optim.SGD(decoder1.parameters(), lr=learning_rate)\n",
    "training(encoder1, decoder1, encoder1_optimizer, decoder1_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And next, we use a greedy method to generate a target sentence based on source sentence. Each time, we selected the word wich has the maximum probability untile the decoder generate an ending token 'EOS'. However, a best choice each time does not guarantte the most-likely sentence in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_greedy(encoder, decoder, sentence, max_length=10):\n",
    "    input_variable = Variable(torch.LongTensor(sentence).view(-1, 1))\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    #encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        #encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "    \n",
    "    #Set the inital value as SOS token\n",
    "    decoder_input = Variable(torch.LongTensor([[0]]))  # SOS\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoded_words = []\n",
    "\n",
    "    #Greedy method\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "        top_value, top_index = decoder_output.data.topk(1)\n",
    "        #Get the index of the word\n",
    "        ni = top_index[0][0]\n",
    "        if ni == 1:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(ger_ix2vocab[ni])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        #decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "\n",
    "    print(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_greedy(encoder1, decoder1, test_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:myPytorch]",
   "language": "python",
   "name": "conda-env-myPytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
